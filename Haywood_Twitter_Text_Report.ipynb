{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64867ec3",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8425e92",
   "metadata": {},
   "source": [
    "Importing pandas is making the data being able to be read in the format of a data table.Importing json is the type of file format we are using. Importing requests is importing the ability for Jupyter Notebook to take the query and access the data on Twitter. Lastly, importing the urllib is importing the python library so that any code ran in python applies to Jupyter Notebook.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e178da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b45610",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = pd.read_csv(\"Twitter_keys_922.txt\", header = 0, sep ='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5ecd5",
   "metadata": {},
   "source": [
    "The bearer token acts as the keys to a house. Instead of listing the tokens here in the document, the code is saying, in order to access the bearer token, it can be found in this text document.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token['Bearer Token'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bdde5b",
   "metadata": {},
   "source": [
    "From the text file, the code is saying to read the 0th term and that is the bearer token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb233b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Authorization': 'Bearer {}'.format(bearer_token['Bearer Token'].iloc[0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5526f",
   "metadata": {},
   "source": [
    "Headers is making itself equal the authorization of the bearer token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'https://api.twitter.com/2/tweets/search/recent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20f3dc",
   "metadata": {},
   "source": [
    "The endpoint is the url to where the requests will be taken from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89329c",
   "metadata": {},
   "source": [
    "## Defining Query and Query Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ab6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_fields = 'username'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42226510",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_fields = 'author_id,lang,public_metrics,created_at'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59bdb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = '(autism ADHD) lang:en -is:retweet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05133ac",
   "metadata": {},
   "source": [
    "For this assignemnt, I wanted to research what is being said on Twitter about autism and ADHD. Both neurodivergencies overlap a lot and I was curious whether or not the tweets would be more about personal experiences or other topics including adhd and autism. My limitations with this query are I would like to narrow down more on the topic specifically of females with adhd and or autism. I had a hard time doing so as well as making the query functional. Another limitation was at first making my query too specfic. I would like to gather data on a specific topic and by doing this I think that I would need to collect the data multiple times and or run different functions with the query to collect the amount of responses I need. I did not get a chance to understand how to collect the 300 responses, but I did collect around 200. \n",
    "Above the query,the query parameters are being established. Within the query the following things will be included: username, author, language, time created,\n",
    "The author id helps to identify the user, lang is the langauge of the tweet, time created is the time the tweet was created, and the public metrics is accessed at the time that the query is accessed.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b856154",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url = endpoint + '?query={}&tweet.fields={}&max_results=100'.format(query_text,tweet_fields,user_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeff00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(query_url, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c55e5",
   "metadata": {},
   "source": [
    "Assigning the response to the twitter API requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e866f",
   "metadata": {},
   "source": [
    "## First set of responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13254975",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656e83d",
   "metadata": {},
   "source": [
    "Assigning the response dictionary to be the responses of the query.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a1b38",
   "metadata": {},
   "source": [
    "Listing the keys listed in the response dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0bcef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc406b",
   "metadata": {},
   "source": [
    "Listing the responses in a more orderly way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6566627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response_dict['meta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ccee4",
   "metadata": {},
   "source": [
    "Listing the result counts that are found within the query and its parameters as well as the newest tweet and oldest tweet.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e23aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df =pd.DataFrame(response_dict['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7b6a0",
   "metadata": {},
   "source": [
    "The response data frame is being assigned to the response data of the query.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50910d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025a4b6",
   "metadata": {},
   "source": [
    "Even though that this data table helps visualize the data more, I would have liked to learn and understand how to dive into the data even further. I would have liked to learn how to remove certain columns from the dataframe as well as from certain columns, expand on that data and extract things such as the usernames and the text itself. It would have been helpful to learn how to make the data text from the tweets, appear in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f714ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_metrics_df = pd.DataFrame(list(response_df['public_metrics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_final = pd.concat([response_df, public_metrics_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fdbf8f",
   "metadata": {},
   "source": [
    "## Second set of responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_query_url = query_url + \"&next_token={}\".format(response_dict['meta']['next_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_response = requests.get(next_query_url, headers = headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40a334",
   "metadata": {},
   "source": [
    "The line of code accesses the next page of tweets that are generated from the query.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b82bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d48dd0",
   "metadata": {},
   "source": [
    "Next page of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_response_dict = json.loads(next_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b443675",
   "metadata": {},
   "source": [
    "Assigning the next dictionary of responses to the next page of responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf147011",
   "metadata": {},
   "source": [
    "## Setting up a Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7390087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twt_recent_search (query, num_pages, headers):\n",
    "    response_list = []\n",
    "    next_token = ''\n",
    "    for i in range(0, num_pages):\n",
    "        if i > 0:\n",
    "            this_query = query + \"&next_token={}\".format(next_token)\n",
    "            \n",
    "        else:\n",
    "            this_query = query\n",
    "            \n",
    "        this_response = requests.get(this_query, headers = headers)\n",
    "        print(this_response.status_code)\n",
    "        this_response_dict = json.loads(this_response.text)\n",
    "        response_list.append(this_response_dict)\n",
    "        next_token = this_response_dict ['meta']['next_token']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return response_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77724bae",
   "metadata": {},
   "source": [
    "This long cell is defining that the recent search will include the query parameters and will result in a certain number of pages. \n",
    "i is representing the number of pages. As long as i is greater than 0, then more responses will be received. If not, the query will remain the same and will not go on to a next page. \n",
    "this response is assigning the requests and the query to and in turn, will print the response status code if all the parameters are correct within them. From there, then the responses will be listed as the next token or the next page of responses. I believe that I understand what this loop is doing, however I am not quite sure of its importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507df852",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_responses = twt_recent_search (query_url, 4, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27327606",
   "metadata": {},
   "source": [
    "4 represents the number of pages needed in order to collect all the data I need. I would have liked to learn how to be able to request 4 pages of the data and be able to see it visualized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bba14c",
   "metadata": {},
   "source": [
    "## Start of the results from the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = pd.DataFrame.from_records(my_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list(results_1['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cbafc7",
   "metadata": {},
   "source": [
    "The data list is assigning the list of results and from that list of results, specifically the data key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_of_dfs = [pd.DataFrame(x) for x in data_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93544311",
   "metadata": {},
   "source": [
    "The data list of data frames is being assigned to the Pandas Dataframe of x. I am not entirely sure what the x represents in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400bfe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df= pd.concat(data_list_of_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71a9af",
   "metadata": {},
   "source": [
    "The data data frame is being assigned to the pandas concat data list of dataframes. I am not sure what concat means or what the specific function is. I looked up the term and I am still confused. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266b911",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135e7f3",
   "metadata": {},
   "source": [
    "Overall, this assignment was quite difficult for me in terms of understanding the code. I learned that I need to start asking more questions in class if I am confused or at the very least to clarify my understanding of the code and its importance and function. I would have liked to been able to get to the point of starting to analyze my data to see what kind of tweets exist that specifically pertain to women with adhd and or autism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef5072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
